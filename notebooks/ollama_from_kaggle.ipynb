{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6776c7bf",
   "metadata": {},
   "source": [
    "# Run prompts on Ollama from Kaggle\n",
    "    \n",
    "This notebook shows **two ways** to use Ollama from Kaggle:\n",
    "    \n",
    "**A) Connect to a _remote_ Ollama endpoint (recommended).**  \n",
    "Point the code to an Ollama server you control (e.g., your workstation with a tunnel, a cloud VM behind HTTPS+auth, etc.). Kaggle can then call it with `requests`.\n",
    "\n",
    "**B) (Experimental) Run Ollama _inside Kaggle_.**  \n",
    "Install Ollama, start the server in the notebook, and run a **small** model (e.g., `llama3.1:8b` or `phi3:mini`). Downloading a 20B model is likely too large/slow for Kaggle session limits.\n",
    "    \n",
    "> **Note:** Kaggle notebooks run in an isolated VM. They **cannot reach your local machine** unless you expose it (e.g., via reverse proxy/tunnel). Also, make sure to enable **Internet** in the notebook settings when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72fcd7",
   "metadata": {},
   "source": [
    "## A) Connect to a remote Ollama endpoint (recommended)\n",
    "\n",
    "1. Expose your local or cloud Ollama at a secure URL (e.g., behind nginx + basic auth or a tunnel like Cloudflare Tunnel/ngrok).  \n",
    "2. Set the base URL and model name below.  \n",
    "3. Run the cell to send prompts.\n",
    "\n",
    "> If you're exposing your **local** machine: start Ollama on your box and publish `http://localhost:11434` through a tunnel to a public **HTTPS** endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "# --- CONFIG ---\n",
    "# Point to your REMOTE Ollama endpoint (HTTPS strongly recommended)\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"https://your-ollama.example.com\")  # e.g., from a tunnel or reverse proxy\n",
    "OLLAMA_API = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "MODEL = os.getenv(\"OLLAMA_MODEL\", \"gpt-oss:20b\")  # or any model available on your server\n",
    "\n",
    "# Optional: basic auth or bearer token, if you secured your endpoint\n",
    "BASIC_AUTH = None  # e.g., ('user', 'pass')\n",
    "BEARER_TOKEN: Optional[str] = None\n",
    "\n",
    "def run_prompt_remote(prompt: str, model: str = MODEL, stream: bool = False) -> str:\n",
    "    headers = {}\n",
    "    if BEARER_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {BEARER_TOKEN}\"\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"stream\": stream}\n",
    "    r = requests.post(OLLAMA_API, json=payload, headers=headers, auth=BASIC_AUTH, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    if stream:\n",
    "        # If your endpoint streams, you'll need to iterate over lines/chunks here.\n",
    "        # For simplicity we assume non-streaming for now.\n",
    "        raise NotImplementedError(\"Streaming parse not implemented in this snippet.\")\n",
    "    return r.json()[\"response\"]\n",
    "\n",
    "# Example usage:\n",
    "resp = run_prompt_remote(\"Give me 3 fun facts about llamas.\")\n",
    "print(resp[:1000])  # print first 1000 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0775f6",
   "metadata": {},
   "source": [
    "## B) (Experimental) Run Ollama inside Kaggle\n",
    "\n",
    "This may or may not work smoothly depending on session limits. **Use a small model** to avoid long downloads and disk/memory issues.\n",
    "\n",
    "### Steps\n",
    "1. Enable **Internet** for the notebook.\n",
    "2. Run the install cell.\n",
    "3. Start the Ollama server in the background.\n",
    "4. Pull a small model (e.g., `llama3.1:8b`).\n",
    "5. Call the local API at `http://127.0.0.1:11434`.\n",
    "\n",
    "> Tip: If the server isn't ready yet, add a short sleep/retry loop before calling the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install Ollama (requires Internet). If this fails, re-run after enabling Internet in settings.\n",
    "# You may be running as root in Kaggle; sudo may not be necessary.\n",
    "!curl -fsSL https://ollama.com/install.sh | sh || echo \"Install script failed — check Internet setting.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Start the Ollama server in the background for this session\n",
    "import subprocess, time, os, signal\n",
    "\n",
    "# Start server\n",
    "server = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "# Give it a moment to come up\n",
    "time.sleep(3)\n",
    "\n",
    "# Optional: print a few lines of server logs (non-blocking peek)\n",
    "for _ in range(5):\n",
    "    line = server.stdout.readline().strip()\n",
    "    if not line:\n",
    "        break\n",
    "    print(line)\n",
    "\n",
    "print(\"Ollama server started (attempted).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Pull a SMALL model to keep things quick. (20B models are usually impractical in Kaggle sessions.)\n",
    "# Choose ONE of the below:\n",
    "!ollama pull llama3.1:8b || echo \"Pull failed — check disk/Internet.\"\n",
    "# !ollama pull phi3:mini || echo \"Pull failed — check disk/Internet.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f743edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Call the local endpoint from Python\n",
    "import requests, time\n",
    "\n",
    "LOCAL_API = \"http://127.0.0.1:11434/api/generate\"\n",
    "\n",
    "def run_prompt_local(prompt: str, model: str = \"llama3.1:8b\", stream: bool = False) -> str:\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"stream\": stream}\n",
    "    for attempt in range(10):\n",
    "        try:\n",
    "            r = requests.post(LOCAL_API, json=payload, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}. Retrying in 3s...\")\n",
    "            time.sleep(3)\n",
    "    raise RuntimeError(\"Could not reach local Ollama server. Check logs above.\")\n",
    "\n",
    "print(run_prompt_local(\"Summarize Kaggle in one paragraph.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bd87f",
   "metadata": {},
   "source": [
    "## Notes & Tips\n",
    "\n",
    "- **Why remote is recommended:** Kaggle VMs are ephemeral and have strict time/disk constraints. Hosting Ollama elsewhere ensures faster startup and predictable performance.\n",
    "- **Securing your endpoint:** Put Ollama behind a reverse proxy (nginx/Caddy) with HTTPS and auth; or use a tunnel (Cloudflare Tunnel) with access policies.\n",
    "- **Using your 20B model:** If you must use `gpt-oss:20b`, host it **remotely** and call it from Kaggle (Path A).\n",
    "- **Streaming:** If your endpoint streams, adapt the request to `stream=True` and iterate over `r.iter_lines()` (server must support streaming)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
